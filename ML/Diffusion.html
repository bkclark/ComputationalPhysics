
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Generative Diffusion Models &#8212; Computing in Physics (Phy446)</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Overview" href="../TI/Overview.html" />
    <link rel="prev" title="Restricted Boltzmann Machines" href="RBM.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Computing in Physics (Phy446)</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Setting up
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../GettingStarted.html">
   Getting Setup
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Cellular Automata
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../CellularAutomata/CellularAutomata.html">
   Cellular Automata
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../CellularAutomata/Sand.html">
   Sand
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../CellularAutomata/OtherAutomata.html">
   Other Interesting Automata
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Quantum Computing
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../QC/Overview.html">
   Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../QC/0-DiracNotation.html">
   Dirac Notation
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../QC/1a-QuantumComputingSimulator.html">
   QC Simulator II
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../QC/1b-QuantumComputingSimulator.html">
     QC Simulator I(abcd)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../QC/NonAtomicGates.html">
   Non-atomic gates
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../QC/PhaseEstimation.html">
   Phase estimation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../QC/QFT.html">
   Quantum Fourier Transform
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../QC/Shor-Overview.html">
   Shor’s Algorithm
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../QC/Shor-Classical.html">
     Shor’s Algorithm (classically)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../QC/Shor-Quantum.html">
     Quantum Matrix
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../QC/ModularMultiplication.html">
     Modular Multiplication
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../QC/Shor-QuantumCircuit.html">
     Shor’s Algorithm
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Quantum Computing (extensions)
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../QC/Gates.html">
   Gates
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../QC/ClassicalGates.html">
     Classical Gates
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../QC/ControlledGates.html">
     Controlled Gates
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../QC/Universal.html">
     Gates for any Unitary
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../QC/BQPinPSPACE.html">
   BQP in PSPACE
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Ising Model
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Ising/Overview.html">
   Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ising/IsingModel.html">
   Simulating an Ising Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ising/Measure.html">
   Measuring the Ising Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ising/RG.html">
   The Renormalization Group
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ising/ParallelTempering.html">
   Extra Credit: Parallel Tempering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ising/SimulatedAnnealing.html">
   Extra Credit: Simulated Annealing
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Machine Learning
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Overview.html">
   Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Hopfield.html">
   Hopfield Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="RBM.html">
   Restricted Boltzmann Machines
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Generative Diffusion Models
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Topological Insulators
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../TI/Overview.html">
   Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../TI/Lattice.html">
   Lattices
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../TI/TightBinding.html">
   Tight Binding Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../TI/ChernInsulators.html">
   Topological Insulators
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#diffusion">
   Diffusion
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#approach-1">
     Approach 1
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#approach-2">
     Approach 2
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#approach-3">
     Approach 3
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-next-step-faster-diffusing">
   The Next Step: Faster Diffusing
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#undiffusing">
   Undiffusing
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#training">
   Training
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#prompts">
   Prompts
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Generative Diffusion Models</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#diffusion">
   Diffusion
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#approach-1">
     Approach 1
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#approach-2">
     Approach 2
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#approach-3">
     Approach 3
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-next-step-faster-diffusing">
   The Next Step: Faster Diffusing
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#undiffusing">
   Undiffusing
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#training">
   Training
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#prompts">
   Prompts
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pylab</span> <span class="k">as</span> <span class="nn">plt</span>
</pre></div>
</div>
</div>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="generative-diffusion-models">
<h1>Generative Diffusion Models<a class="headerlink" href="#generative-diffusion-models" title="Permalink to this headline">#</a></h1>
<p>Our goal in this assignment is to understand generative models like stable diffusion.  In stable diffusion, you type a string and it generates a picture of that string.  For this assignment, we are going to get to 70% of that understanding.</p>
<div class="section" id="diffusion">
<h2>Diffusion<a class="headerlink" href="#diffusion" title="Permalink to this headline">#</a></h2>
<p>We are going to start by thinking about diffusion. Consider a pollen particle which is being buffeted by air.  If we weren’t paying attention to the air, but instead just the pollen particle, what we would expect to see is that the pollen particle would ocassionally jump in random directions as the air is hitting it.  We might model this by saying that at time <span class="math notranslate nohighlight">\(t\)</span> the particle is at <span class="math notranslate nohighlight">\(x_t = x_{t-1} +\sqrt{\delta} z\)</span> where <span class="math notranslate nohighlight">\(z \sim N(0,1)\)</span> is a random guassian number with unit standard deviation.   Now if the pollen was also in some sort of potential <span class="math notranslate nohighlight">\(E(x)\)</span> (say a gravitational field or a harmonic oscillator) we would expect it to also drift with the force induced by this potential.  This motivates the Langevin equation</p>
<div class="math notranslate nohighlight">
\[x_t = x_{t-1} + \frac{\delta}{2} F(x_{t-1})+ \sqrt{\delta} z \]</div>
<p>where <span class="math notranslate nohighlight">\(z\sim N(0,1)\)</span> is a random gaussian number with unit standard deviation,  <span class="math notranslate nohighlight">\(F(x_{t-1}) =  -\nabla_x E(x_{t-1}) = \nabla_x\log p(x_{t-1})\)</span>   and <span class="math notranslate nohighlight">\(p(x_{t-1}) = \exp[-E(x_{t-1})]\)</span>.  While we will start with <span class="math notranslate nohighlight">\(x\)</span> being in one dimension, we will generically allow <span class="math notranslate nohighlight">\(x\)</span> to be in arbitrary dimension.</p>
<p>Notice that this is a Markov chain:  the new position <span class="math notranslate nohighlight">\(x_{t}\)</span> only depends on the current position <span class="math notranslate nohighlight">\(x_{t-1}\)</span> and not any of the other times.</p>
<p>Because it is a Markov chain it has to have a stationary distribution.  It turns out (in the limit of small-ish <span class="math notranslate nohighlight">\(\delta\)</span>), the probability distribution you end up with is <span class="math notranslate nohighlight">\(\exp[-E(x)]\)</span>.  This will be a key property of the Langevin equation for our purposes.  There are three ways we can verify this:</p>
<ul class="simple">
<li><p>Simulate the Langevin equation and check its probability distribution</p></li>
<li><p>Make another Markov chain that we know has the right probabilty distribution and show that it is equivalent to Langevin dynamics</p></li>
<li><p>Demonstrate that the Langevin equation has the right dynamics</p></li>
</ul>
<div class="section" id="approach-1">
<h3>Approach 1<a class="headerlink" href="#approach-1" title="Permalink to this headline">#</a></h3>
<p>We will start with the first approach.  Write a function</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">ForwardDiffusion</span><span class="p">(</span><span class="n">x_0</span><span class="p">,</span><span class="n">T</span><span class="p">,</span> <span class="n">delta</span><span class="p">):</span>
  <span class="c1"># do stuff</span>
  <span class="k">return</span> <span class="n">locations</span>
</pre></div>
</div>
<p>which takes an initial condition <code class="docutils literal notranslate"><span class="pre">x_0</span></code> and an integer number of steps <span class="math notranslate nohighlight">\(T\)</span>, and a step-size <span class="math notranslate nohighlight">\(\delta\)</span>.  Let’s use a simple harmonic oscillator as the energy - i.e. <span class="math notranslate nohighlight">\(E=x^2/2\)</span>.  It should return the list of locations that it has been at (later when we work with more dimensions we will switch to just returning the last location).</p>
<div class="caution admonition">
<p class="admonition-title">Grading</p>
<p>After you’ve written this function, go ahead and run it out to <span class="math notranslate nohighlight">\(T=10000\)</span>.  Plot a histogram of the distribution of locations that you visited <em>ignoring the first 1000 steps</em>. These first 1000 steps are just the transient.   Check that the distribution that you arrive at is</p>
<div class="math notranslate nohighlight">
\[P(x) = \frac{1}{\sqrt{2\pi}} \exp[-x^2/2]\]</div>
</div>
</div>
<div class="section" id="approach-2">
<h3>Approach 2<a class="headerlink" href="#approach-2" title="Permalink to this headline">#</a></h3>
<p>We can alternatively start with a Markov chain that we know the stationary distribution to - i.e. the Metropolis Markov chain.</p>
<p>In this section we are trying to reason about why the forward diffusion equation generated a probability distribution <span class="math notranslate nohighlight">\(P(x) \propto \exp[-E(x)]\)</span>.  The algorithm that we described is clearly a Markov chain - i.e. the probabilistic locations of the next step only depend on the current locations.  Therefore there needs to be some stationary distribution.  Our goal is just to figure out which one.  Unfortunately, we don’t know how to easily reason about the stationary distributions.  Let’s see if instead we can start with a Markov chain that we do understand and which generates the right distribution and massage it into the equation above.</p>
<p>One Markov chain we do understand is the Metropolis method.  In the Metropolis method, we know that we should choose a move from <span class="math notranslate nohighlight">\(x\)</span> to <span class="math notranslate nohighlight">\(x'\)</span> with probability <span class="math notranslate nohighlight">\(T(x \rightarrow x')\)</span> and then we should accept that move with probability</p>
<div class="math notranslate nohighlight">
\[ A(x\rightarrow x') = \frac{\pi(x')}{\pi(x)} \frac{T(x' \rightarrow x)}{T(x \rightarrow x')}\]</div>
<p>Let’s start with moving with simply a guassian with standard deviation <span class="math notranslate nohighlight">\(\sqrt{\delta}\)</span>.  Then</p>
<div class="math notranslate nohighlight">
\[T(x \rightarrow x') = T(x' \rightarrow x) = \exp\left[-\frac{(x-x')^2}{2\delta}\right]\]</div>
<p>Then we should accept with probability</p>
<div class="math notranslate nohighlight">
\[A(x \rightarrow x') = \exp[-(E(x')-E(x))]\]</div>
<p>This will give us a Markov chain which samples the probability distribution <span class="math notranslate nohighlight">\(p(x) \propto \exp[-E(x)]\)</span> but unfortunately doesn’t look like our dynamics above.</p>
<div class="caution admonition">
<p class="admonition-title">Grading</p>
<p>Nonetheless, go ahead and implement this markov chain using the quadratic energy <span class="math notranslate nohighlight">\(E(x)=x^2/2\)</span> and verify that it gives the right distribution.</p>
</div>
<p>Now, we are going to go ahead and try to massage our Metropolis MCMC into something that looks more like our target dynamics.  Let us now try a new sample probability using</p>
<div class="math notranslate nohighlight">
\[T(x \rightarrow x') = \exp\left[-\frac{((x+\delta/2 F(x))-x')^2}{2\delta}\right]\]</div>
<p>What this probability distribution tells us is that we should first drift our particle from <span class="math notranslate nohighlight">\(x \rightarrow x+\frac{\delta}{2} F(x)\)</span> and then sample a gaussian with standard deviation <span class="math notranslate nohighlight">\(\sqrt{\delta}\)</span> from that point.  This, in fact, is the Langevin dynamics described above.</p>
<p>Using Metropolis, let’s work out the acceptance probability <span class="math notranslate nohighlight">\(A(x \rightarrow x')\)</span> for this new choice.</p>
<div class="math notranslate nohighlight">
\[ \frac{T(x' \rightarrow x)}{T(x \rightarrow x')} = \exp\left[\delta (x-x')[F(x)+F(x')] +(\delta/2)^2 (F(x)^2 - F(x')^2)  \right]\]</div>
<p>We will find that the acceptance ratio <span class="math notranslate nohighlight">\(A(x \rightarrow x')\)</span> approaches 1 if the potential is linear over a range of <span class="math notranslate nohighlight">\(\sqrt{\delta}\)</span>. One can see this analytically.   Instead let’s go ahead and test this by running a variational Monte Carlo using this improved move.    Notice, that this is guaranteed to sample the right distribution.  If the acceptance ratio is 1 (or essentially 1) then we are guaranteed that the dynamics above actually do sample the correct thing.</p>
<div class="caution admonition">
<p class="admonition-title">Grading</p>
<p>Go ahead and show that the acceptance ratio is 1 by running this in a Monte Carlo and computing the acceptance ratio.</p>
<p>In addition, you can check this by choosing a series of <span class="math notranslate nohighlight">\(x\)</span> points (no Monte Carlo involved), choose an <span class="math notranslate nohighlight">\(x'\)</span> by the Langevin equation and verify that the acceptance ratio for this would be essentially 1.0 if you made this move. Use <span class="math notranslate nohighlight">\(\delta =0.05\)</span></p>
</div>
</div>
<div class="section" id="approach-3">
<h3>Approach 3<a class="headerlink" href="#approach-3" title="Permalink to this headline">#</a></h3>
<p>The two previous approaches were essentially showing that the Langevin equation gives us the stationary distribution we want numerically.  We can also show this analytically.</p>
<p>Starting from a generalized version of the Langevin eqution, <span class="math notranslate nohighlight">\(x(t + \Delta t) = x(t) + l(t) + F\gamma\Delta t\)</span>, where <span class="math notranslate nohighlight">\(l(t)\)</span> is essentially any reasonable symmetric random perturbation.</p>
<p>You can show that this satisfies the diffusion equation,</p>
<div class="math notranslate nohighlight">
\[ \frac{\partial \rho}{\partial t} = D \frac{\partial^2 \rho}{\partial x^2} - \gamma F \frac{\partial \rho}{\partial x}\]</div>
<p>A nice exposition of this is in <a class="reference external" href="https://sethna.lassp.cornell.edu/StatMech/EntropyOrderParametersComplexity20.pdf">James Sethna’s Stat Mech Book</a> around equations 2.7.</p>
<p>It’s pretty easy from this equation to see that it corresponds to the drift, diffusion step of the Langevin equation - i.e. formally the solution of <span class="math notranslate nohighlight">\(\frac{\partial \rho}{\partial t} = (\hat{A} + \hat{B})\rho\)</span> for small t is <span class="math notranslate nohighlight">\(\rho(t) = \exp[A t] \exp[B t]\)</span> which directly corresponds to the drift and diffusion term.</p>
<p>It’s also easy to work out what the stationary distribution of this equation as <span class="math notranslate nohighlight">\(\frac{\partial \rho}{\partial t} = 0\)</span> giving us that</p>
<div class="math notranslate nohighlight">
\[  D \frac{\partial^2 \rho}{\partial x^2}  = \gamma F \frac{\partial \rho}{\partial x}\]</div>
<p>which is satisfied by</p>
<div class="math notranslate nohighlight">
\[ \rho(x) = \mathcal C \exp\left( -\frac{\gamma}{D} E(x) \right)\]</div>
<p>Interestingly, this is very closely related to a method for simulating quantum ground states - diffusion Monte Carlo.</p>
</div>
</div>
<div class="section" id="the-next-step-faster-diffusing">
<h2>The Next Step: Faster Diffusing<a class="headerlink" href="#the-next-step-faster-diffusing" title="Permalink to this headline">#</a></h2>
<p>So far, we’ve learned that the Langevin dynamics (in the limit of small-ish <span class="math notranslate nohighlight">\(\delta\)</span>) are a Markov chain which ends up in the probability distribution proportional to <span class="math notranslate nohighlight">\(p(x) \propto \exp[-E(x)]\)</span>.</p>
<p>We are now going to focus explicitly on the energy functional for a harmonic oscillator <span class="math notranslate nohighlight">\(E(x) = x^2/2\)</span> and we are going to generalize our diffusion to allow different step sizes at each step (as well as do a somewhat unfortunate change of variables - this will ensure that we are consistent with both the Langevin literature (from above) and the diffusion literature (now below)).</p>
<p>Using this energy function, we get</p>
<div class="math notranslate nohighlight">
\[x_t= x_{t-1} \left(1- \frac{\delta}{2}\right) + \sqrt{\delta} z\]</div>
<p>We know are going to let <span class="math notranslate nohighlight">\(\delta\)</span> be a function of time giving us</p>
<div class="math notranslate nohighlight">
\[x_t= x_{t-1} \left(1- \frac{\delta_t}{2}\right) + \sqrt{\delta_t} z\]</div>
<p>and then let <span class="math notranslate nohighlight">\(\beta_t = \delta_t\)</span> and <span class="math notranslate nohighlight">\(\sqrt{1-\beta_t} = \left(1- \frac{\delta_t}{2}\right)\)</span></p>
<p>This last identification is true in the limit of small <span class="math notranslate nohighlight">\(\delta_t\)</span> (just Taylor expand the exponential) and is commonly what is done in the literature so we are going to do it here.  There is probably no deep fundamental reason to do so.</p>
<p>This leaves us</p>
<div class="math notranslate nohighlight">
\[x_t= x_{t-1} \sqrt{1-\beta_t} + \sqrt{\beta_t} z\]</div>
<p>We will now let <span class="math notranslate nohighlight">\(\beta_t = 0.0001 + (0.02-0.0001)/200 t\)</span><br />
This can be generated in python by just doing
<code class="docutils literal notranslate"><span class="pre">beta</span> <span class="pre">=</span> <span class="pre">np.linspace(0.0001,</span> <span class="pre">0.02,</span> <span class="pre">timesteps,dtype=np.float32)</span></code></p>
<p>Update your function
<code class="docutils literal notranslate"><span class="pre">def</span> <span class="pre">ForwardDiffusion(x_0,beta_t,timeSteps):</span></code></p>
<p>to run forward diffusion over a certain number of time-steps using a time-dependent standard deviation <span class="math notranslate nohighlight">\(\beta_t\)</span>.</p>
<div class="caution admonition">
<p class="admonition-title">Grading</p>
<p>Make a plot for five runs a graph of location at time <span class="math notranslate nohighlight">\(x(t)\)</span> vs <span class="math notranslate nohighlight">\(t\)</span>.</p>
<p>This time it doesn’t make sense to collect the probability distribution over many steps because the probability distribution changes over time.<br />
Instead, run your function 10000 times out to a time-step of T=199 and graph a histogram of the resulting distribution x(199).  Plot a theory curve on top of it showing that you get the correct probability distribution.</p>
</div>
<p>Now, we would like to produce a new function which produces the same probability distribution but does it quickly.  In other words, you want to produce the same probability distribution after some number of time steps but which doesn’t require taking each and every step to get there (i.e. you just want to be able to jump to step 150).  It turns out that this possible because the sum of a bunch of random gaussian steps is a random gaussian step.</p>
<p>If we let
$<span class="math notranslate nohighlight">\(\alpha_t = 1-\beta_t\)</span><span class="math notranslate nohighlight">\(
and 
\)</span><span class="math notranslate nohighlight">\(\overline{\alpha_t}= \prod_{i=1}^t \alpha_i\)</span>$</p>
<p>then you can jump to step <span class="math notranslate nohighlight">\(T\)</span> from initial point <span class="math notranslate nohighlight">\(x_0\)</span> by doing</p>
<div class="math notranslate nohighlight">
\[\sqrt{\overline{\alpha_t}} x_0 + \sqrt{1-\overline{\alpha_t}} z\]</div>
<div class="caution admonition">
<p class="admonition-title">Grading</p>
<p>Go ahead and write a
<code class="docutils literal notranslate"><span class="pre">def</span> <span class="pre">ForwardDiffusionFast(x_0,alpha_bar_t,timeSteps):</span></code>
which quickly generates the same distribution as ForwardDiffusion.  Plot a histogram of ForwardDiffusionFast out to T=200 and check that you get the same probability distribution as ForwardDiffusion</p>
</div>
</div>
<div class="section" id="undiffusing">
<h2>Undiffusing<a class="headerlink" href="#undiffusing" title="Permalink to this headline">#</a></h2>
<p>Our next step is to figure out how to undiffuse.</p>
<p>In our forward diffusion process we have samples <span class="math notranslate nohighlight">\(x_0\)</span> from an initial distriution <span class="math notranslate nohighlight">\(p_{init}(x_0)\)</span> which we forward diffuse</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">x0</span> <span class="o">=</span> <span class="n">SamplePInit</span><span class="p">()</span>
<span class="n">xt</span> <span class="o">=</span> <span class="n">ForwardDiffusionFast</span><span class="p">(</span><span class="n">x_0</span><span class="p">,</span><span class="n">T</span><span class="p">,</span><span class="n">alpha_bar</span><span class="p">)</span>
</pre></div>
</div>
<p>giving us a (largely gaussian) final distribution <span class="math notranslate nohighlight">\(p_{final}\)</span>.</p>
<p>Now, we’d like to start with samples <span class="math notranslate nohighlight">\(x_t\)</span> from <span class="math notranslate nohighlight">\(p_{final}\)</span> and write a function <code class="docutils literal notranslate"><span class="pre">Undiffuse(xt)</span></code> which returns a sample <span class="math notranslate nohighlight">\(x_0\)</span> which is sampled from <span class="math notranslate nohighlight">\(p_{init}\)</span> - i.e.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">x0</span> <span class="o">=</span> <span class="n">SamplePInit</span><span class="p">()</span>
<span class="n">xt</span> <span class="o">=</span> <span class="n">ForwardDiffusionFast</span><span class="p">(</span><span class="n">x_0</span><span class="p">,</span><span class="n">T</span><span class="p">,</span><span class="n">alpha_bar</span><span class="p">)</span>
<span class="n">new_x0</span> <span class="o">=</span> <span class="n">Undiffuse</span><span class="p">(</span><span class="n">xt</span><span class="p">)</span> 
<span class="c1"># new_x0 doesn&#39;t have to be x_0 but if we histogram $new_x0$ and $x_0$ those histograms should be the same</span>
</pre></div>
</div>
<p>In some sense, this should be possible - the laws of physics don’t know about the direction in time.  In practice though, you don’t often see things undiffuse even if we reverse the force (i.e. after a drop of food coloring diffuses in a cup, it doesn’t undiffuse by turning it upside-down.) The reason for this is that we aren’t actually successfully reversing the directions of “all the air molecules bouncing off the pollen” even if we do reverse the force on the pollen.  Nonetheless, if we are careful (and have the right information) we can get this undiffusion to happen.   Mathematically, what we need to do is figure out how to (stochastiaclly) run the Langevin Markov chain backwards.</p>
<p>The Langevin markov chain is a rule which tells us, given <span class="math notranslate nohighlight">\(x_{t-1}\)</span> how we should stochastically choose <span class="math notranslate nohighlight">\(x_t\)</span> - i.e. it is defined by <span class="math notranslate nohighlight">\(p(x_t | x_{t-1})\)</span>.</p>
<p>To reverse it, we would like to figure out <span class="math notranslate nohighlight">\(p(x_{t-1} | x_{t})\)</span>.   It turns out we can’t quite do this.  Instead, what we are going to do is get a rule for <span class="math notranslate nohighlight">\(p(x_{t-1} | x_{t},x_0)\)</span>.  In practice, then to run our Markov chain backwards we start with <span class="math notranslate nohighlight">\(x_t\)</span>, guess the <span class="math notranslate nohighlight">\(x_0\)</span> that would have generated that <span class="math notranslate nohighlight">\(x_t\)</span>, and then use our rule to get <span class="math notranslate nohighlight">\(x_{t-1}\)</span>.  We can then do this over and over again decreasing <span class="math notranslate nohighlight">\(t\)</span> until we get to <span class="math notranslate nohighlight">\(t=0\)</span>.  Actually it will turn out to be slightly preferable to guess the random number <span class="math notranslate nohighlight">\(z_g^t\)</span> used in your fast diffusion function (i.e. the result of the call to <code class="docutils literal notranslate"><span class="pre">np.random.randn()</span></code>) then <span class="math notranslate nohighlight">\(x_0\)</span>.  But since you can get <span class="math notranslate nohighlight">\(x_0\)</span> given <span class="math notranslate nohighlight">\(z_g^t\)</span> and <span class="math notranslate nohighlight">\(x_t\)</span> it’s equivalent.</p>
<div class="dropdown admonition">
<p class="admonition-title">Derivation</p>
<p>Let’s go ahead and derive the rule for <span class="math notranslate nohighlight">\(p(x_{t-1}|x_t,x_0)\)</span>.</p>
<p>First start by looking at <span class="math notranslate nohighlight">\(p(x_t, x_{t-1}, x_0)\)</span>.   We can expand this out using Bayes theorem as</p>
<p><span class="math notranslate nohighlight">\(p(x_t, x_{t-1}, x_0) = p(x_t | x_{t-1}, x_0) p(x_{t-1}|x_0) p(x_0)\)</span></p>
<p>and</p>
<p><span class="math notranslate nohighlight">\(p(x_t, x_{t-1},x_0) = p(x_{t-1}| x_t,x_0) p(x_{t}|x_0)p(x_0)\)</span></p>
<p>Some algebra then gives us
<span class="math notranslate nohighlight">\(p(x_{t-1}|x_t,x_0) = p(x_t| x_{t-1},x_0)\frac{p(x_{t-1}|x_0)}{p(x_t|x_0)}\)</span></p>
<p>Notice the first term on the rhs doesn’t depend on <span class="math notranslate nohighlight">\(x_0\)</span> (it’s a Markov chain and so if you tell me <span class="math notranslate nohighlight">\(x_{t-1}\)</span> I don’t also need <span class="math notranslate nohighlight">\(x_0\)</span>).</p>
<p>Using our fast diffusion this gives us</p>
<p><span class="math notranslate nohighlight">\(p(x_{t-1}|x_t,x_0) = \exp\left[-\frac{1}{2}\left(\frac{(x_t-\sqrt{\alpha_t}x_{t-1})^2}{\beta_t}  + \frac{ (x_{t-1}-\sqrt{\overline{\alpha_{t-1} } } x_0)^2}{1-\overline{\alpha_{t-1}}} - \frac{( x_t - \sqrt{\overline{\alpha_t}}x_0)^2}{1-\overline{\alpha_t}} \right)   \right]\)</span></p>
<p>You can simplify this by rewriting this as
$<span class="math notranslate nohighlight">\(p(x_{t-1} | x_t, x_0) = \frac{1}{\sqrt{2\pi \tilde{\beta_t}}} \exp\left[-\frac{(x-\tilde{\mu}_t)^2}{2\tilde{\beta_t}}\right]\)</span>$</p>
<p>where
$<span class="math notranslate nohighlight">\(\tilde{\mu}_t= \frac{\sqrt{\alpha_t}(1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t} \mathbf{x}_t + \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1 - \bar{\alpha}_t} \mathbf{x}_0 \)</span>$</p>
<p>and
$<span class="math notranslate nohighlight">\(\tilde{\beta_t} = \frac{1-\overline{\alpha}_{t-1}}{1-\overline{\alpha}_t} \beta_t\)</span>$</p>
<p>This tells us that our undiffusion step should be (after guess <span class="math notranslate nohighlight">\(x_0\)</span>)</p>
<div class="math notranslate nohighlight">
\[x_{t-1} = x_t + \tilde{\mu}_t + N(0,1) \sqrt{\tilde{\beta_t}}\]</div>
<p>(technical note: we don’t do the N(0,1) step when <span class="math notranslate nohighlight">\(t=0\)</span>)</p>
<p>Now from our fast diffusion we have that</p>
<p><span class="math notranslate nohighlight">\(x_0 = \frac{1}{\sqrt{\overline{\alpha}_t}} (x_t - \sqrt{1-\overline{\alpha}_t}z^g_t)\)</span>
where <span class="math notranslate nohighlight">\(z^g_t\)</span> is our guess for the noise from foward diffusion (i.e. instead of guess <span class="math notranslate nohighlight">\(x_0\)</span> we can guess the noise we used to get <span class="math notranslate nohighlight">\(x_t\)</span>).</p>
<p>We can plug <span class="math notranslate nohighlight">\(x_0\)</span> into our diffusion step.  Using some algebra we end up with the following:</p>
</div>
<p>When we work this all out the undiffusing step gives us (after we guess the random noise)</p>
<div class="math notranslate nohighlight">
\[x_{t-1}=\frac{1}{\sqrt{\alpha_t}} (x_t - s_t * z^g_t) + \delta_{t,0} \sqrt{\tilde{\beta_t}} N(0,1)  \]</div>
<p>where <span class="math notranslate nohighlight">\(\delta_{t,0}\)</span> is 1 unless <span class="math notranslate nohighlight">\(t=0\)</span>, <span class="math notranslate nohighlight">\(z^g_t\)</span> is your guess for the full noise used to diffuse from <span class="math notranslate nohighlight">\(t=0\)</span>, and the noise scale</p>
<div class="math notranslate nohighlight">
\[s_t=\frac{1-\alpha_t}{\sqrt{1-\overline{\alpha_t}}}\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[\tilde{\beta_t} = \frac{1-\overline{\alpha}_{t-1}}{1-\overline{\alpha}_t} \beta_t\]</div>
<p>Let’s go ahead and write some functions now to get this working:</p>
<p>Write an <code class="docutils literal notranslate"><span class="pre">Undiffuse(xt)</span></code> function which takes <span class="math notranslate nohighlight">\(x_t\)</span> and (stochastically) returns <span class="math notranslate nohighlight">\(x_{t-1}\)</span>. In this function you are going to call <code class="docutils literal notranslate"><span class="pre">GuessZ(xt)</span></code> which you will also write.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">Undiffuse</span></code> function is generic.  We will also write a <code class="docutils literal notranslate"><span class="pre">SamplePInit()</span></code> function.  For our first example, we will make it always return 0.4.  In other words, our probability distribution is <span class="math notranslate nohighlight">\(p(0.4)=1\)</span> and <span class="math notranslate nohighlight">\(p(x\neq 0.4)=0\)</span>.</p>
<p>Also go ahead and write <code class="docutils literal notranslate"><span class="pre">GuessZ</span></code>.   In this case, since you (secretly) know that <span class="math notranslate nohighlight">\(x_0\)</span> is always <span class="math notranslate nohighlight">\(0.4\)</span> you can probably do a really good job of having guessZ successfully guess the right random noise :)</p>
<div class="caution admonition">
<p class="admonition-title">Grading</p>
<p>Once these functions are written, let’s work on the undiffusion.</p>
<ul class="simple">
<li><p>Run your undiffusion 5 times starting all from a single diffused point.  Graph <span class="math notranslate nohighlight">\(x(t)\)</span> vs <span class="math notranslate nohighlight">\(t\)</span></p></li>
<li><p>Run your undiffusion many times out to T=0.  Show that you get a delta function at 0.4</p></li>
<li><p>Run your diffusion from T=0 to T=125 and your undiffusion from T=200 to T=125 and plot the histograms showing they are the same.</p></li>
</ul>
</div>
<p>We now want to make life a little harder.  We are going to have a new probability distribution.</p>
<div class="math notranslate nohighlight">
\[p(0.4) = 0.8\]</div>
<div class="math notranslate nohighlight">
\[p(-0.6) = 0.2\]</div>
<div class="math notranslate nohighlight">
\[p(\text{anything else}) = 0\]</div>
<p>Write the new <code class="docutils literal notranslate"><span class="pre">SampleFromInitP</span></code> function.</p>
<p>Now we also need a new <code class="docutils literal notranslate"><span class="pre">guessZ(xt)</span></code> function.  Notice that this is much trickier and doesn’t always have an obvious answer.  If I tell you what <span class="math notranslate nohighlight">\(x_t\)</span> is, you don’t really know which <span class="math notranslate nohighlight">\(x_0\)</span> it came from (maybe 0.4 and maybe -0.6).  Therefore, you don’t know whether to report <span class="math notranslate nohighlight">\(z_{0.4}\)</span> (i.e. the random guess from 0.4) or to report <span class="math notranslate nohighlight">\(z_{-0.6}\)</span>.  It turns out the right thing to do is to report is the weighted average</p>
<div class="math notranslate nohighlight">
\[\frac{0.8 z_a \exp(-z_a^2/2) + 0.2 z_b \exp(-z_b^2/2)}{0.8\exp(-z_a^2/2)+0.2\exp(-z_b^2/2)}\]</div>
<div class="caution admonition">
<p class="admonition-title">Grading</p>
<p>Do the undiffusion from this probabaility distribution.  Plot the same plots from the undiffusion from a single delta function.</p>
<p>In addition, notice that at each step in your diffusion you are making a guess for <span class="math notranslate nohighlight">\(z\)</span>.  This corresponds to a guess for <span class="math notranslate nohighlight">\(x_0\)</span>.  For ten runs, graph the guess as a function of <span class="math notranslate nohighlight">\(t\)</span>.</p>
</div>
</div>
<div class="section" id="training">
<h2>Training<a class="headerlink" href="#training" title="Permalink to this headline">#</a></h2>
<p>So far we’ve managed to do undiffusion where we know what the right guess for the random number <span class="math notranslate nohighlight">\(z_g\)</span> is.  But in practice we don’t always know this.  Now, we’d like to work in a situation where we don’t know this.  Instead, we are going to train a neural network which learns the noise.</p>
<p>To do this, we are going to use pytorch.  To generate a simple neural network using pytorch, we can have</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="n">n_input</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_out</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">1</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_input</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">),</span>
                      <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
                      <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">),</span>
                      <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
                      <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_out</span><span class="p">),</span>
                      <span class="p">)</span>
<span class="n">net</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">3.0</span><span class="p">,</span><span class="mi">4</span><span class="p">]))</span>  <span class="o">&lt;--</span><span class="n">This</span> <span class="n">runs</span> <span class="n">the</span> <span class="n">network</span> <span class="k">with</span> <span class="n">a</span> <span class="n">noise</span> <span class="n">of</span> <span class="mi">3</span> <span class="n">at</span> <span class="n">time</span><span class="o">-</span><span class="n">step</span> <span class="mi">4</span>
</pre></div>
</div>
<p>Now, we need to learn how to use pytorch to train a network to match the noise.  Essentially what we are going to do is the following:</p>
<ul class="simple">
<li><p>Pick a random <span class="math notranslate nohighlight">\(t\)</span></p></li>
<li><p>Get from your ForwardDiffusionFast function both the noise (&lt;–this is a new thing you have to return) and the noisy data.</p></li>
<li><p>Have your network guess the noise.  Using a loss-function modify your network to match the noise more closely.</p></li>
</ul>
<p>Here is the general framework for pytorch optimization.  You have to define some optimization pieces.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span> 

<span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">200000</span><span class="p">):</span>
  <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
  <span class="n">x0</span><span class="o">=</span> <span class="n">SamplePInit</span><span class="p">()</span>
  <span class="c1"># Choose a random time t</span>
  
  <span class="c1"># call your FowardDiffusionFast (make sure you return the noisyData and the noise)</span>
  <span class="n">noisyData</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">noisyData</span><span class="p">,</span><span class="n">t</span><span class="p">])</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="c1"># include the time for the data</span>
  <span class="n">noise</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">noise</span><span class="p">])</span> <span class="c1">#make it so pytorch reads the noise</span>

  <span class="n">loss</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">(</span><span class="n">noise</span><span class="p">,</span><span class="n">net</span><span class="p">(</span><span class="n">noisyData</span><span class="p">))</span>
  <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
  <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>  
</pre></div>
</div>
<p>Note that <code class="docutils literal notranslate"><span class="pre">loss.item()</span></code> gives you the loss.</p>
<p>Fill out your optimization.  Run it and you should then have a net which guesses your random noise.</p>
<div class="caution admonition">
<p class="admonition-title">Grading</p>
<p>Train your network.  Plot the loss as a function of training step.  You may have to do a window averaging over approximately 100 steps to generate this plot.</p>
</div>
<p>Now in your undiffuse, you can make your <code class="docutils literal notranslate"><span class="pre">guess=net(torch.tensor([float(x_t),t]))</span></code> as opposed to calling model guess. Go ahead and make this replacement and then run your undiffusion generating both the standard 10 samples of undiffusion as well as the histogram as well as the best guess.</p>
<div class="caution admonition">
<p class="admonition-title">Grading</p>
<p>Generate these plots.</p>
</div>
</div>
<div class="section" id="prompts">
<h2>Prompts<a class="headerlink" href="#prompts" title="Permalink to this headline">#</a></h2>
<p>Finally in programs such as stable diffusion, you give the program a prompt and ask it to produce that image - i.e. astronaut on a horse.  In our simple example here, we are going to also give our simulation a prompt:  we will work with a simple version of this telling it either “left” or “right” to ask it to either find the point below zero or above zero.</p>
<p>To accomplish this, during the training we need to give our network not only the diffused location <span class="math notranslate nohighlight">\(x_t\)</span> and the time <span class="math notranslate nohighlight">\(t\)</span> but also the prompt-information.  It needs to give it this information both during training and undiffusing.  That way, during training it will learn to be biased toward the right location conditioned on the prompt.  We will embed as “left” equals -1 and “right” equals 1 - i.e. <code class="docutils literal notranslate"><span class="pre">embedPrompt</span> <span class="pre">=</span> <span class="pre">-1</span> <span class="pre">if</span> <span class="pre">prompt==&quot;left&quot;</span> <span class="pre">else</span> <span class="pre">1</span></code></p>
<p>Modify your network to take an extra input:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">n_input</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_out</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">1</span>  <span class="c1"># &lt;-- all I&#39;ve changed is n_input is now 3</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_input</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">),</span>
                      <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
                      <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">),</span>
                      <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
                      <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_out</span><span class="p">),</span>
                      <span class="p">)</span>
</pre></div>
</div>
<p>Now, when you call the network you need to call it as
<code class="docutils literal notranslate"><span class="pre">net(torch.tensor([float(x_t),t,embedPrompt]))</span></code></p>
<p>During training to get the embedded prompt, you want to check if it’s less then 0 (send it the embeddedPrompt for left) or more then 0 (send it the embeddedPrompt for 1).</p>
<p>During undiffusion, you then have to decide whether your prompt is “left” or “right” and then run it.</p>
<div class="caution admonition">
<p class="admonition-title">Grading</p>
<p>Modify your code to do “prompting.”  Train your network and then produce two sets of our three typical plots:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(x(t)\)</span> vs <span class="math notranslate nohighlight">\(t\)</span></p></li>
<li><p>histogram of final outcome</p></li>
<li><p>guess of <span class="math notranslate nohighlight">\(x(0)\)</span> as a function of <span class="math notranslate nohighlight">\(t\)</span></p></li>
</ul>
<p>One set should be when the prompt is “left” and one set should be when the prompt is “right.”</p>
</div>
<p>Please continue onto <a class="reference internal" href="Diffusion2.html"><span class="doc std std-doc">3.2 Diffusion Models - page 2</span></a></p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./ML"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="RBM.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Restricted Boltzmann Machines</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../TI/Overview.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Overview</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Bryan Clark<br/>
  
      &copy; Copyright 2023.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>